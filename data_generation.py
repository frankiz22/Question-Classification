import subprocess

commands = [
    "git clone https://github.com/ArmelRandy/Self-instruct.git",
    "pip install -r Self-instruct/requirements.txt",
    "pip install -r requirements.txt"
]

for command in commands:
    subprocess.run(command, shell=True)

from sklearn.model_selection import train_test_split
from transformers import (AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, DataCollatorWithPadding,
                            TrainingArguments, AutoModelForSequenceClassification)
from datasets import Dataset
import matplotlib.pyplot as plt
import torch
import pandas as pd
import numpy as np
import json
import torch.nn as nn
import evaluate

from typing import List



def create_seed_task(X_train: pd.DataFrame, 
                     filename_seed: str) -> None:
    """
    Create seed task from a Dataset
    
    Args:
    - X_train: dataframe from which seed task are created
    - filename_seed : path to the file that contains the seed task in json format
    """
    
    seed_list = []
    
    for index in X_train.index:
        task = X_train.loc[index]
        instruction, output = task['text'], task['label']
        seed_list.append({'instruction':instruction, 'output':output})
        
    
    with open(filename_seed, "w") as json_file:
        json.dump(seed_list, json_file)

def predict(model: AutoModelForCausalLM, classes: List[str], text: str):
    """
    Generates text based on input using an AutoModelForCausalLM.

    Args:
    - model (AutoModelForCausalLM): AutoModelForCausalLM model for text generation.
    - classes (List[str]): List of classes for the prediction.
    - input_text (str): Input text to generate predictions.

    Returns:
    - str: Predicted text generated by the model.
    """
    prompt = '\n\n'.join(X_train.apply(lambda x : 'text: ' + x['text'] + '\nlabel: ' + x['label'], axis = 1 ))
    total_prompt = prompt + '\n\ntext: ' + text + '\nlabel: '
    input_ids = tokenizer(total_prompt, return_tensors="pt").to(device)
    sample = model.generate(**input_ids, max_new_tokens = 6)
    result_string = tokenizer.decode(sample[0])
    result_string = result_string[len(total_prompt):]
    
    for classe in classes:
        if classe in result_string:
            return classe
    return 'out_of_scope'
    
def self_instruct(
    filename_seed: str = "data/generation.json",
    output_instruction_path: str = "data/output.jsonl",
    output_data_path: str = "data/generated.csv",
    num_instructions_to_generate: int = 400,
    model_name_or_path: str = "mistralai/Mistral-7B-v0.1",
    num_prompt_instructions: int = 8,
    request_batch_size: int = 8,
    num_prompt_synthetic_instructions: int = 2,
    max_new_tokens: int = 4096,
    temperature: float = 0.8,
    top_p: float = 0.95,
    num_beams: int = 1,
    repetition_penalty: float = 1.2,
    threshold: float = 0.7,
    seed: int = 42
) -> None:
    """
    Generate instructions using self-instruct.

    Args:
    - filename_seed (str): Path to the file containing seeds for generation.
    - output_data_path (str): Output path to save the generated data.
    - num_instructions_to_generate (int): Total number of instructions to generate.
    - model_name_or_path (str): Name or path of the language generation model.
    - num_prompt_instructions (int): Number of instructions used as prompts.
    - request_batch_size (int): Batch size for generation requests.
    - num_prompt_synthetic_instructions (int): Number of synthetic instructions for each prompt.
    - max_new_tokens (int): Maximum number of new tokens in each generated instruction.
    - temperature (float): Temperature for token sampling.
    - top_p (float): Cumulative probability for token selection.
    - num_beams (int): Number of beams for decoding search.
    - repetition_penalty (float): Penalty for token repetition.
    - threshold (float): Selection threshold for generation scores.
    - seed (int): Seed for reproducibility.

    Returns:
    - None: The function does not return anything.
    """   
    
    command = '''python self_instruct_main.py \
        --seed_tasks_path=filename_seed \
        --output_data_path=output_data_path  \
        --num_instructions_to_generate=num_instructions_to_generate \
        --template_name better \
        --format 2 \
        --model_name_or_path=model_name_or_path \
        --num_prompt_instructions=num_prompt_instructions \
        --request_batch_size=request_batch_size \
        --num_prompt_synthetic_instructions=num_prompt_synthetic_instructions \
        --max_new_tokens=max_new_tokens \
        --temperature=temperature \
        --top_p=top_p \
        --num_beamsnum_beams=num_beamsnum_beams \
        --repetition_penalty=repetition_penalty\
        --threshold=threshold \
        --seed=seed \
        --keep_programming'''
    subprocess.run(command, shell=True)
    

def generation(
    X_train: pd.DataFrame,
    filename_seed: str = "newdata/generation.json",
    output_instruction_path: str = "newdata/output.jsonl",
    output_data_path: str = "newdata/generated.csv",
    num_instructions_to_generate: int = 8,
    model_name_or_path: str = "mistralai/Mistral-7B-v0.1",
    num_prompt_instructions: int = 8,
    request_batch_size: int = 8,
    num_prompt_synthetic_instructions: int = 2,
    max_new_tokens: int = 4096,
    temperature: float = 0.8,
    top_p: float = 0.95,
    num_beams: int = 1,
    repetition_penalty: float = 1.2,
    threshold: float = 0.7,
    seed: int = 42
) -> None:
    """
    Generate instructions using a language generation model.

    Args:
    - filename_seed (str): Path to the file containing seeds for generation.
    - output_instruction_path (str): Output path to save the generated instruction with self-instruct.
    - output_data_path (str): Output path to save the generated data.
    - num_instructions_to_generate (int): Total number of instructions to generate.
    - model_name_or_path (str): Name or path of the language generation model.
    - num_prompt_instructions (int): Number of instructions used as prompts.
    - request_batch_size (int): Batch size for generation requests.
    - num_prompt_synthetic_instructions (int): Number of synthetic instructions for each prompt.
    - max_new_tokens (int): Maximum number of new tokens in each generated instruction.
    - temperature (float): Temperature for token sampling.
    - top_p (float): Cumulative probability for token selection.
    - num_beams (int): Number of beams for decoding search.
    - repetition_penalty (float): Penalty for token repetition.
    - threshold (float): Selection threshold for generation scores.
    - seed (int): Seed for reproducibility.

    Returns:
    - None: The function does not return anything.
    """
    
    create_seed_task(X_train, filename_seed)
    
    
    command = '''python self_instruct_main.py \
    --seed_tasks_path=filename_seed \
    --output_data_path=output_instruction_path \
    --num_instructions_to_generate=num_instructions_to_generate \
    --template_name better \
    --format 2 \
    --model_name_or_path=model_name_or_path \
    --num_prompt_instructions=num_prompt_instructions \
    --request_batch_size=request_batch_size \
    --num_prompt_synthetic_instructions=num_prompt_synthetic_instructions \
    --max_new_tokens=max_new_tokens \
    --temperature=temperature \
    --top_p=top_p \
    --num_beamsnum_beams=num_beamsnum_beams \
    --repetition_penalty=repetition_penalty \
    --threshold=threshold \
    --seed=seed \
    --keep_programming'''
    
    subprocess.run(command, shell=True)
    
    #Loading generated data    
    with open(output_instruction_path, 'r') as file:
        data = file.read()

    data_generated = pd.read_json(data, lines=True)
    data_generated = data_generated[['instruction', 'output']]
    data_generated = data_generated.rename(columns={'instruction': 'text', 'output': 'label'})

    #loading model for prediction
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.bfloat16)    
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
    model = model.to(device)

    #Cleaning data: Make sure that the prediction of the model in few shot correspond to the prediction given during self-instruct    
    idx_to_remove = []
    classes = data_generated['label'].unique()
    
    for index in data_generated.index:
        to_evaluate = data_generated.loc[index]
        text = to_evaluate['text']
        label = to_evaluate['label']
        pred = predict(model, classes, text)
        if pred != label:
            print(f'text: {text}, prediction: {pred}, label: {label}')
            idx_to_remove.append(index)
    df = data_generated.drop(idx_to_remove)
    
    #save the final dataset
    df.to_csv(output_data_path, index = False)